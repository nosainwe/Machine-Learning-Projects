{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0OqavhMKfDXs2rFBFJ307",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nosainwe/Machine-Learning-Projects/blob/main/Project%203%20-%20Implement%20URL%20phishing%20detection%20with%20Logistic%20Regression%20and%20TF-IDF%20vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJpMzIMJqyv8",
        "outputId": "624483dd-141a-45d2-9cb6-30a8c45b5486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 92.18%\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import re  # Regular expression operations\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # For vectorizing text data\n",
        "from sklearn.linear_model import LogisticRegression  # For training the model\n",
        "from sklearn.model_selection import train_test_split  # For splitting the data into training and test sets\n",
        "import pandas as pd  # For handling data as DataFrame\n",
        "import random  # For shuffling data randomly\n",
        "import pickle  # For saving the model and vectorizer to files\n",
        "\n",
        "# Define the url_cleanse function before using it in TfidfVectorizer\n",
        "def url_cleanse(url):\n",
        "    \"\"\"\n",
        "    Function to clean and preprocess the URL string:\n",
        "    1. Converts URL to lowercase.\n",
        "    2. Removes unwanted parts like http://, https://, www.\n",
        "    3. Removes special characters while keeping alphanumeric, dots, and hyphens.\n",
        "    4. Splits the URL into tokens based on dots, slashes, or hyphens for analysis.\n",
        "    \"\"\"\n",
        "    url = url.lower()  # Convert URL to lowercase\n",
        "    url = re.sub(r'http[s]?://', '', url)  # Remove 'http://' or 'https://'\n",
        "    url = re.sub(r'www\\.', '', url)  # Remove 'www.'\n",
        "    url = re.sub(r'[^a-z0-9\\s.-]', '', url)  # Remove all non-alphanumeric characters except dots and hyphens\n",
        "    tokens = re.split(r'[./-]', url)  # Split the URL by dots, slashes, or hyphens\n",
        "    return tokens  # Return the tokens for further analysis\n",
        "\n",
        "# Correct the file path (adjust according to your setup)\n",
        "input_url = '/content/phishing_site_urls.csv'\n",
        "\n",
        "# Read the CSV file properly with the correct delimiter\n",
        "data_csv = pd.read_csv(input_url, sep=',', on_bad_lines='skip')  # Load the dataset from the CSV file\n",
        "\n",
        "# Shuffle the rows by converting the DataFrame to a list\n",
        "data_list = data_csv.values.tolist()  # Convert the DataFrame to a list of lists\n",
        "random.shuffle(data_list)  # Shuffle the data randomly to avoid any order bias\n",
        "\n",
        "# Convert shuffled data back to DataFrame if needed (to preserve column names)\n",
        "data_df = pd.DataFrame(data_list, columns=data_csv.columns)\n",
        "\n",
        "# Split columns into 'y' (labels) and 'inputurls' (features)\n",
        "y = [d[1] for d in data_list]  # Extract the label (e.g., 'bad' or 'good') assuming itâ€™s in the second column\n",
        "inputurls = [d[0] for d in data_list]  # Extract the URLs from the first column\n",
        "\n",
        "# Optionally, you can reassign them back to pandas DataFrame if necessary\n",
        "data_df = pd.DataFrame(data_list, columns=data_csv.columns)  # Recreate DataFrame with shuffled data\n",
        "\n",
        "# Initialize the vectorizer and logistic regression model\n",
        "url_vectorizer = TfidfVectorizer(tokenizer=url_cleanse)  # Use the custom 'url_cleanse' function for tokenization\n",
        "l_regress = LogisticRegression()  # Logistic regression model to classify URLs\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "x_train, x_test, y_train, y_test = train_test_split(inputurls, y, test_size=0.2, random_state=42)  # 80% train and 20% test data\n",
        "\n",
        "# Fit the vectorizer on the training URLs\n",
        "x_train_vectorized = url_vectorizer.fit_transform(x_train)  # Transform the training URLs into a matrix of TF-IDF features\n",
        "\n",
        "# Train the logistic regression model on the vectorized training data\n",
        "l_regress.fit(x_train_vectorized, y_train)  # Fit the model on the training data\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "x_test_vectorized = url_vectorizer.transform(x_test)  # Transform the test URLs using the fitted vectorizer\n",
        "l_score = l_regress.score(x_test_vectorized, y_test)  # Evaluate the accuracy of the model on the test data\n",
        "\n",
        "# Print the score as a percentage\n",
        "print(\"Score: {:.2f}%\".format(100 * l_score))  # Output the accuracy percentage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the URL vectorizer and logistic regression model to files\n",
        "file1 = \"model.pkl\"  # File name for the logistic regression model\n",
        "with open(file1, 'wb') as f:\n",
        "    pickle.dump(l_regress, f)  # Save the trained model\n",
        "\n",
        "file2 = \"vector.pkl\"  # File name for the URL vectorizer\n",
        "with open(file2, 'wb') as f2:\n",
        "    pickle.dump(url_vectorizer, f2)  # Save the vectorizer\n",
        "\n",
        "# To use the saved models for prediction\n",
        "with open(file1, 'rb') as f1:\n",
        "    lgr = pickle.load(f1)  # Load the logistic regression model\n",
        "\n",
        "with open(file2, 'rb') as f2:\n",
        "    url_vectorizer = pickle.load(f2)  # Load the URL vectorizer\n",
        "\n",
        "# Example of predicting for new URLs\n",
        "inputurls = ['hackthebox.eu', 'facebook.com']  # Example URLs for prediction (replace with actual test data)\n",
        "x = url_vectorizer.transform(inputurls)  # Transform the input URLs using the loaded vectorizer\n",
        "y_predict = l_regress.predict(x)  # Predict the legitimacy of the URLs\n",
        "\n",
        "# Print the results\n",
        "print(inputurls)  # Display the input URLs\n",
        "print(y_predict)  # Display the predicted labels for the URLs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98aJzuu1tV7n",
        "outputId": "3947ba9b-faa4-4559-eb56-fb61f5aac459"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yamleg.com', 'facebook.com']\n",
            "['good' 'good']\n"
          ]
        }
      ]
    }
  ]
}
